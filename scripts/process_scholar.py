#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤„ç†å•ä¸ªå­¦è€…çš„æ–‡çŒ®åˆ†æè„šæœ¬
"""

import os
import sys
import argparse
import shutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core import Config, BibTexParser, PublicationExtractor, RankingMatcher
from utils import DataProcessor, ReportGenerator


class ScholarProcessor:
    """å­¦è€…æ–‡çŒ®å¤„ç†å™¨"""
    
    def __init__(self, user_id, config_file=None):
        """
        åˆå§‹åŒ–å­¦è€…å¤„ç†å™¨
        
        Args:
            user_id: ç”¨æˆ·IDï¼ˆDBLP IDï¼‰
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.user_id = user_id
        self.config = Config(config_file)
        self.paths = self.config.get_user_paths(user_id)
        
        # åˆ›å»ºç”¨æˆ·ç›®å½•
        self.config.create_user_directories(user_id)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.bib_parser = BibTexParser()
        
        # åˆ›å»ºPublicationExtractorçš„é…ç½®å­—å…¸
        extractor_config = {
            'timeout': self.config.get('network.timeout', 120),
            'sleep_interval': self.config.get('network.sleep_interval', 3),
            'proxies': self.config.get_proxies()
        }
        self.publication_extractor = PublicationExtractor(extractor_config)
        
        self.ranking_matcher = RankingMatcher(
            ccf_file=self.config.get('data.ccf_file'),
            cas_file=self.config.get('data.cas_file')
        )
        self.data_processor = DataProcessor()
        self.report_generator = ReportGenerator()
    
    def process_bibliography(self, bib_file_path=None):
        """
        å¤„ç†æ–‡çŒ®ä¿¡æ¯
        
        Args:
            bib_file_path: BibTeXæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        if bib_file_path is None:
            bib_file_path = self.paths['bib_file']
        
        # å¦‚æœbibæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„
        if not os.path.exists(bib_file_path):
            print(f"BibTeXæ–‡ä»¶ä¸å­˜åœ¨: {bib_file_path}")
            return None
        
        print(f"å¼€å§‹å¤„ç†å­¦è€… {self.user_id} çš„æ–‡çŒ®...")
        
        # 1. è§£æBibTeXæ–‡ä»¶
        print("\n" + "-" * 50)
        print("ğŸ“„ ç¬¬ä¸€æ­¥ï¼šè§£æBibTeXæ–‡ä»¶")
        print("-" * 50)
        df = self.bib_parser.parse_file(bib_file_path)
        
        # åˆ†æDataFrame
        analysis = self.bib_parser.analyze_dataframe(df)
        print(f"âœ… è§£æå®Œæˆ: {analysis['total_papers']} ç¯‡è®ºæ–‡")
        
        # 2. å»é‡å¤„ç†
        print("\n" + "-" * 50)
        print("ğŸ”„ ç¬¬äºŒæ­¥ï¼šè®ºæ–‡å»é‡å¤„ç†")
        print("-" * 50)
        df_deduplicated = self.data_processor.deduplicate_papers(df, verbose=False)
        
        # ä¿å­˜å»é‡åçš„è§£æç»“æœ
        self.bib_parser.save_results(df_deduplicated, self.paths['processed_dir'])
        
        # 3. æå–å‡ºç‰ˆç‰©ä¿¡æ¯
        print("\n" + "-" * 50)
        print("ğŸ“Š ç¬¬ä¸‰æ­¥ï¼šæå–å‡ºç‰ˆç‰©ä¿¡æ¯")
        print("-" * 50)
        valid_papers, publication_counts, publication_types = self.publication_extractor.extract_unique_publication_abbrs(df_deduplicated)
        
        if not publication_counts:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å‡ºç‰ˆç‰©")
            return None
        
        # æ‰“å°å¤„ç†æ‘˜è¦
        self.report_generator.print_processing_summary(
            df, df_deduplicated, publication_counts, publication_types, None
        )
        
        # 4. è·å–DBLPå‡ºç‰ˆç‰©ä¿¡æ¯
        print("\n" + "-" * 50)
        print("ğŸŒ ç¬¬å››æ­¥ï¼šè·å–DBLPå‡ºç‰ˆç‰©ä¿¡æ¯")
        print("-" * 50)
        
        # è®¡ç®—éœ€è¦æŸ¥è¯¢çš„æœŸåˆŠæ•°é‡
        journal_count = sum(1 for pub_abbr in publication_counts if publication_types.get(pub_abbr) == 'journal')
        
        if journal_count > 0:
            print(f"ğŸ” éœ€è¦æŸ¥è¯¢ {journal_count} ä¸ªæœŸåˆŠçš„æ ‡é¢˜å’ŒISSNä¿¡æ¯")
            print(f"ğŸ“‹ ä¼šè®®è®ºæ–‡ç›´æ¥é€šè¿‡ç¼©å†™åŒ¹é…CCFåˆ†çº§ï¼Œæ— éœ€æŸ¥è¯¢DBLP")
            print(f"â±ï¸  é¢„è®¡è€—æ—¶: {journal_count * 3} ç§’ï¼ˆæ¯ä¸ªæœŸåˆŠçº¦3ç§’ï¼‰")
        else:
            print("ğŸ“‹ å…¨éƒ¨ä¸ºä¼šè®®è®ºæ–‡ï¼Œæ— éœ€æŸ¥è¯¢DBLP")
        
        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(current, total, pub_abbr, paper_count):
            """æ˜¾ç¤ºDBLPä¿¡æ¯è·å–è¿›åº¦"""
            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            progress = (current / total) * 100
            
            # åˆ›å»ºè¿›åº¦æ¡
            bar_length = 30
            filled_length = int(bar_length * current // total)
            bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
            
            # ä»å¸¦å‰ç¼€çš„é”®ä¸­æå–åŸå§‹ç¼©å†™
            if pub_abbr.startswith('journal_'):
                display_abbr = pub_abbr[8:]  # ç§»é™¤ 'journal_' å‰ç¼€
            else:
                display_abbr = pub_abbr
            
            # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
            print(f"\rğŸ” è¿›åº¦: [{bar}] {progress:.1f}% | æœŸåˆŠ: {display_abbr} ({paper_count}ç¯‡) | å‰©ä½™: {total - current}", end="", flush=True)
            
            # å®Œæˆåæ¢è¡Œ
            if current == total:
                print()
        
        # ä½¿ç”¨å…¨å±€DBLPç¼“å­˜æ–‡ä»¶
        global_cache_file = self.config.get_global_cache_file()
        publications_info = self.publication_extractor.batch_extract_publication_info(
            publication_counts, 
            publication_types,
            cache_file=global_cache_file,
            progress_callback=progress_callback
        )
        
        # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        success_count = sum(1 for info in publications_info.values() if info.get('success', False))
        print(f"âœ… DBLPä¿¡æ¯è·å–å®Œæˆ: {success_count}/{len(publications_info)} æˆåŠŸ")
        
        # 5. åŒ¹é…å‡ºç‰ˆç‰©åˆ†åŒº
        print("\n" + "-" * 50)
        print("ğŸ† ç¬¬äº”æ­¥ï¼šåŒ¹é…å‡ºç‰ˆç‰©åˆ†åŒº")
        print("-" * 50)
        ranking_results = self.ranking_matcher.batch_match_publications(publications_info)
        
        # 6. ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        print("\n" + "-" * 50)
        print("ğŸ“ˆ ç¬¬å…­æ­¥ï¼šç”Ÿæˆç»Ÿè®¡æ‘˜è¦")
        print("-" * 50)
        detailed_summary = self.ranking_matcher.generate_detailed_summary(ranking_results, publication_counts)
        
        # 7. ä¿å­˜ç»“æœå’Œç»Ÿè®¡åˆ†æ
        print("\n" + "-" * 50)
        print("ğŸ’¾ ç¬¬ä¸ƒæ­¥ï¼šä¿å­˜ç»“æœå’Œç»Ÿè®¡åˆ†æ")
        print("-" * 50)
        
        # ä¸ºæ¯ç¯‡è®ºæ–‡æ·»åŠ CCFå’Œä¸­ç§‘é™¢åˆ†åŒºä¿¡æ¯
        df_with_rankings = self.data_processor.add_ranking_info_to_papers(df_deduplicated, ranking_results)
        
        # ä½¿ç”¨æŠ¥å‘Šç”Ÿæˆå™¨ç”Ÿæˆå’Œä¿å­˜å®Œæ•´æŠ¥å‘Š
        report_results = self.report_generator.generate_and_save_complete_report(
            df_with_rankings,
            ranking_results,
            publication_counts,
            detailed_summary,
            self.paths['processed_dir'],
            self.data_processor
        )
        
        # è·å–ç»Ÿè®¡ç»“æœ
        ranking_stats = report_results['ranking_stats']
        
        # 8. æ‰“å°æœ€ç»ˆç»Ÿè®¡ç»“æœ
        print("\n" + "-" * 50)
        print("ğŸ“‹ ç¬¬å…«æ­¥ï¼šç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š")
        print("-" * 50)
        self.report_generator.print_final_statistics(detailed_summary, ranking_stats)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰å¤„ç†æ­¥éª¤å®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“ å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {self.paths['processed_dir']}")
        print(f"ğŸ“Š å¯æŸ¥çœ‹ç”Ÿæˆçš„ExcelæŠ¥å‘Šå’ŒJSONç»Ÿè®¡æ–‡ä»¶")
        
        return {
            'df': df_deduplicated,
            'df_with_rankings': df_with_rankings,
            'publication_counts': publication_counts,
            'publication_types': publication_types,
            'publications_info': publications_info,
            'ranking_results': ranking_results,
            'detailed_summary': detailed_summary,
            'ranking_stats': ranking_stats,
            'report_results': report_results
        }
    
    def copy_bib_file(self, source_bib_file):
        """
        å¤åˆ¶BibTeXæ–‡ä»¶åˆ°ç”¨æˆ·ç›®å½•
        
        Args:
            source_bib_file: æºBibTeXæ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(source_bib_file):
            raise FileNotFoundError(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source_bib_file}")
        
        # æ£€æŸ¥æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶æ˜¯å¦ç›¸åŒ
        source_abs_path = os.path.abspath(source_bib_file)
        target_abs_path = os.path.abspath(self.paths['bib_file'])
        
        if source_abs_path == target_abs_path:
            print(f"æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶ç›¸åŒï¼Œè·³è¿‡å¤åˆ¶: {self.paths['bib_file']}")
            return
        
        shutil.copy2(source_bib_file, self.paths['bib_file'])
        print(f"å·²å¤åˆ¶BibTeXæ–‡ä»¶åˆ°: {self.paths['bib_file']}")
    
    def get_user_status(self):
        """è·å–ç”¨æˆ·å¤„ç†çŠ¶æ€"""
        status = {
            'user_id': self.user_id,
            'directories_exist': all(os.path.exists(path) for path in [
                self.paths['base_dir'], 
                self.paths['raw_dir'], 
                self.paths['processed_dir']
            ]),
            'bib_file_exists': os.path.exists(self.paths['bib_file']),
            'processed': os.path.exists(self.paths['summary_json']),
            'global_cache_exists': os.path.exists(self.config.get_global_cache_file())
        }
        return status


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤„ç†å­¦è€…æ–‡çŒ®åˆ†æ')
    parser.add_argument('user_id', help='ç”¨æˆ·ID (DBLP ID)')
    parser.add_argument('--bib-file', help='BibTeXæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--status', action='store_true', help='æŸ¥çœ‹ç”¨æˆ·çŠ¶æ€')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = ScholarProcessor(args.user_id, args.config)
    
    if args.status:
        # æŸ¥çœ‹çŠ¶æ€
        status = processor.get_user_status()
        print(f"ç”¨æˆ·ID: {status['user_id']}")
        print(f"ç›®å½•ç»“æ„: {'å·²åˆ›å»º' if status['directories_exist'] else 'æœªåˆ›å»º'}")
        print(f"BibTeXæ–‡ä»¶: {'å­˜åœ¨' if status['bib_file_exists'] else 'ä¸å­˜åœ¨'}")
        print(f"å¤„ç†çŠ¶æ€: {'å·²å¤„ç†' if status['processed'] else 'æœªå¤„ç†'}")
        print(f"å…¨å±€ç¼“å­˜: {'å­˜åœ¨' if status['global_cache_exists'] else 'ä¸å­˜åœ¨'}")
        return
    
    # å¤åˆ¶BibTeXæ–‡ä»¶ï¼ˆå¦‚æœæä¾›äº†ï¼‰
    if args.bib_file:
        processor.copy_bib_file(args.bib_file)
    
    # å¤„ç†æ–‡çŒ®
    results = processor.process_bibliography()
    
    if results:
        print("å¤„ç†æˆåŠŸå®Œæˆï¼")
    else:
        print("å¤„ç†å¤±è´¥æˆ–æ— æœ‰æ•ˆæ•°æ®")


if __name__ == '__main__':
    main() 